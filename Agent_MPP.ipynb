{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992fea2c-fa9e-4acc-b020-4de7c3512fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c621eca4-9eb2-4a50-8800-af0f403811e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent parameters\n",
    "# agent which can choose any value of investment from 0 to 100 \n",
    "# in risky asset and consume (0,1,2,5,10,20,50,100)% of the wealth\n",
    "\n",
    "class PogChamp:\n",
    "    def __init__(self):\n",
    "\n",
    "        # statespace vector\n",
    "        # time as a percentage of time left in episode \n",
    "        self.state = [x/(100) for x in range(100)]\n",
    "        # actionspace vector\n",
    "        # 1st: pi, the percentage invested in risky asset\n",
    "        # 2nd: consumption percantages of wealth \n",
    "        self.actions = [[(x)/100,y]\n",
    "                        for x in range(101)\n",
    "                        for y in [0.01,0.1,0.5,1]]\n",
    "        # q-table to store state-action pair values\n",
    "        # optimal strategy will choose action to which has the greatest \n",
    "        # q value in the table at the given state\n",
    "        # table is 2 dimension storing all state x action pairs\n",
    "        self.q_table = np.zeros((len(self.state),len(self.actions)))\n",
    " \n",
    "    def action_step(self,wealth, action, t,episode_consumption):\n",
    "        # action taken\n",
    "        pi,consumption = self.actions[action]\n",
    "        # log actions\n",
    "        pi_count[pi]+=1\n",
    "        c_count.append(consumption)\n",
    "        # scale consumption\n",
    "        consumption*=10\n",
    "        episode_consumption.append(wealth*consumption*dt)\n",
    "        # collect reward for actions           \n",
    "        reward = self.get_reward(consumption*wealth*dt,gamma)*pow(rho,-t*dt)\n",
    "        # utility of wealth\n",
    "        if (t+1)== total_days: reward+= agent.get_reward(wealth,gamma)\n",
    "        return pi, reward,consumption,episode_consumption\n",
    "    \n",
    "    \n",
    "    \n",
    "    # formula for calculating how the wealth process develops every timestep\n",
    "    def get_dW(self,W, pi, mu, r, c,dt, sigma):\n",
    "        dZ = np.random.normal()*(np.sqrt(dt))\n",
    "        drift = W*(pi*(mu-r)+r-c)\n",
    "        dw = drift*dt + pi*sigma*W*dZ\n",
    "        return dw\n",
    "\n",
    "\n",
    "    # calculates the reward per unit of consumption according to CRRA formula\n",
    "    def get_reward(self,u, gamma,t = 0):\n",
    "        if gamma == 1:\n",
    "            reward = np.log(u)\n",
    "        else:\n",
    "            reward = (u**(1-gamma))/(1-gamma)\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61115d3a-d13a-4859-b7c8-e986ee1965f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Paramater settings\n",
    "\n",
    "# number of episodes trials\n",
    "# discount rate for rewards\n",
    "# learning rate\n",
    "num_episodes = 50000\n",
    "rho = 1.05\n",
    "alpha = 0.01\n",
    "\n",
    "\n",
    "\n",
    "# # parameters for wealth process\n",
    "\n",
    "# T = Total lifespan to use portfolio\n",
    "# dt = timestep\n",
    "# total_days = total number of days in the lifespan\n",
    "# volatility\n",
    "# drift\n",
    "# riskless rate\n",
    "# investor gamma\n",
    "T = 5\n",
    "dt=1/252\n",
    "total_days = int(T/dt)\n",
    "sigma = 0.2\n",
    "mu = 0.1\n",
    "r= 0.02\n",
    "gamma = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef6c372-7599-4fcc-948c-500b31285ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RL training\n",
    "# activate agent\n",
    "agent = PogChamp()\n",
    "pi_count = {(x/100):0 for x in range(101)}\n",
    "total_consumed  = []\n",
    "final_wealth =[]\n",
    "ep_rewards = []\n",
    "\n",
    "\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    # new state parameters\n",
    "    wealth = 50000\n",
    "    # clear history\n",
    "    discounted_sum = 0\n",
    "    state_action_history = []\n",
    "    rewards_history = []\n",
    "    c_count = []\n",
    "    episode_consumption = []\n",
    "    G= 0 \n",
    "    for t in range(total_days):\n",
    "        if t%10 == 0:\n",
    "            # current state\n",
    "            state = int(np.floor(100*t/total_days))\n",
    "            # get action\n",
    "            prob_exploit = 1-0.5*np.exp(-0.0005*episode)\n",
    "            # exploit\n",
    "            if np.random.random() < prob_exploit:            \n",
    "                action = np.argmax(agent.q_table[state])\n",
    "            # explore\n",
    "            else:\n",
    "                action = np.random.choice(range(len(agent.actions)))\n",
    "            \n",
    "            pi, reward,consumption,episode_consumption = agent.action_step(wealth, action, t,episode_consumption)\n",
    "            # record visit\n",
    "            agent.q_table_visits[state,action] += 1\n",
    "            state_action_history.append([state,action])\n",
    "            # take action and observe the new state and reward from the environment\n",
    "            \n",
    "            # record reward\n",
    "            \n",
    "            rewards_history.append(reward)\n",
    "        else: consumption = 0\n",
    "        \n",
    "        dw = agent.get_dW(wealth, pi, mu, r, consumption, dt, sigma)\n",
    "        wealth += dw\n",
    "\n",
    "    \n",
    "    total_returns = np.cumsum(rewards_history)[::-1]\n",
    "    G = total_returns[0]\n",
    "    for i in range(len(rewards_history)):\n",
    "        s = state_action_history[i][0]\n",
    "        a = state_action_history[i][1]\n",
    "        # update here next lookup formula in videos\n",
    "        agent.q_table[s,a] = agent.q_table[s,a] + alpha*(total_returns[i] - agent.q_table[s,a])\n",
    "    \n",
    "    \n",
    "    # record episode results\n",
    "    ep_rewards.append(G)\n",
    "    total_consumed.append(sum(episode_consumption))\n",
    "    final_wealth.append(wealth)\n",
    "    \n",
    "    # clear history\n",
    "    state_action_history = []\n",
    "    \n",
    "    if (episode+1) % 1000 == 0:\n",
    "        template = \"culumative reward: {:.8f} at episode {}\"\n",
    "        print(template.format(G, episode+1))\n",
    "        \n",
    "mean_rewards = [np.mean(ep_rewards[n-300:n]) if n > 300 else np.mean(ep_rewards[:n]) \n",
    "           for n in range(1, len(ep_rewards))] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100fd117-d606-47d4-9c8a-9d792e385eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(mean_rewards)\n",
    "plt.title('Total Return')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.savefig('Agent Total Return Linear.png')\n",
    "plt.show()  \n",
    "\n",
    "\n",
    "picount = [pi_count[x] for x in pi_count]\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(picount)\n",
    "plt.title('pi frequency')\n",
    "plt.xlabel('pi')\n",
    "plt.ylabel('frequency')\n",
    "plt.savefig('Pi Allocation Histogram MC.png')\n",
    "plt.show()   \n",
    "\n",
    "x = np.linspace(0,1,len(c_count))\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(x,np.array(c_count)/10)\n",
    "plt.title('Sample Consumption Trajectory')\n",
    "plt.xlabel('Percentage of Time Elapsed')\n",
    "plt.ylabel('Proportion of Consumption per Timestep')\n",
    "plt.savefig('Sample Consumption Trajectory.png')\n",
    "plt.show() \n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "(x, y) = np.meshgrid(np.arange(agent.q_table.shape[0])/100, np.arange(agent.q_table.shape[1]))\n",
    "z = abs(agent.q_table)\n",
    "print(x.shape,y.shape,z.shape)\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.set_xlabel('Percent of Time')\n",
    "ax.set_ylabel('Pi and Consumption')\n",
    "ax.set_zlabel('Total Return')\n",
    "surf = ax.plot_surface(x, y, z.T, cmap=cm.coolwarm,\n",
    "                       linewidth=0, antialiased=False)\n",
    "plt.savefig('Agent Q-Table.png')\n",
    "plt.show()\n",
    "\n",
    "tally = \"Total money consumed: {:.2f}\\n\\\n",
    "Total money leftover: {:.2f}\"\n",
    "\n",
    "MoneyC = np.mean(total_consumed)\n",
    "MoneyL = np.mean(final_wealth)\n",
    "print(tally.format(MoneyC,MoneyL))\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
